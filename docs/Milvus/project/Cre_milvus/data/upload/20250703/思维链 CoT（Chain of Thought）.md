# 思维链 CoT（Chain of Thought）

## 什么是Zero-Shot、One-Shot、Few-Shot

Zero-Shot、One-Shot、Few-Shot用来解决训练数据少，模型泛化能力（模型对于未见过的新数据的预测能力）差的问题

这些概念之间的差异在于模型在处理任务时接触到的训练示例数量以及模型从这些示例中学到的能力。Zero-Shot要求模型具有广泛的背景知识和推理能力，而Few-Shot则要求模型能够从少量示例中学习并泛化到新示例。

* **Zero-Shot学习**：训练集中没有某个类别的样本，但是在测试集中出现了这个类别，模型在测试集中遇到这个没有遇到过的样本，仍然可用通过对这个类别的描述，对这个没见过的样本进行分类
* **One-Shot学习**：可以理解为用一条数据去fine-tune(微调)模型。
* **Few-Shot学习**：模型在学习大量数据后，对于一些新类别的数据，只需要少量样本即可快速学习

- **Zero-Shot提示：**模型只根据任务的描述生成响应，不需要任何示例。
- **One-Shot提示：**只提供一个例子。
- **Few-Shot提示：**提供几个例子。在提示中的作用是通过少量样本引导模型对特定任务进行学习和执行，例如通过提供少量风格或主题示例，引导模型产出具有相似风格或主题的创作。

## 什么是COT

思维链（CoT）是一种用于设计 prompt 的方法，即 prompt 中除了有任务的输入和输出外，还包含推理的中间步骤（[中间思维](https://zhida.zhihu.com/search?content_id=233779234&content_type=Article&match_order=1&q=%E4%B8%AD%E9%97%B4%E6%80%9D%E7%BB%B4&zhida_source=entity)）。

COT通过要求/提示模型在输出最终答案之前，**显式输出中间逐步的推理步骤**这一方法来增强大模型的算数、常识和推理的性能，2022 年，在 Google 发布的论文《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》中首次提出，**通过让大模型逐步参与将一个复杂问题分解为一步一步的子问题并依次进行求解的过程可以显著提升大模型的性能。而这一系列推理的中间步骤就被称为思维链（Chain of Thought）**。

区别于传统的 Prompt 从输入直接到输出的映射 <input——>output> 的方式，CoT 完成了从输入到思维链再到输出的映射，即 <input——>reasoning chain——>output>。

**一个完整的包含 CoT 的 Prompt 往往由指令（Instruction），逻辑依据（Rationale），示例（Exemplars）三部分组成**。

### CoT的区分

以是否包含示例为区分，可以将 CoT 分为 Zero-Shot-CoT 与 Few-Shot-CoT，**在上图中，Zero-Shot-CoT 不添加示例而仅仅在指令中添加一行经典的“Let's think step by step”，就可以“唤醒”大模型的推理能力。而 Few-Shot-Cot 则在示例中详细描述了“解题步骤”，让模型照猫画虎得到推理能力**。

### CoT的好处

* **增强了大模型的推理能力**：将计算资源分配到核心解体步骤上
* **增强了大模型的可解释性**：解题过程将帮助我们定位错误
* **增强了大模型的可控性**：避免大模型成为完全的黑盒
* **增强了大模型的灵活性**：仅添加一句“Let's think step by step”，就可以在现有的各种不同的大模型中使用 CoT 方法，同时，CoT 赋予的大模型一步一步思考的能力不仅仅局限于“语言智能”，在科学应用，以及 AI Agent 的构建之中都有用武之地。

### CoT生效

1. **模型规模小会导致 CoT 失效；**
2. **简单的任务 CoT 不会对模型性能带来提升；**
3. **训练数据内部彼此相互联结程度的增加可以提升 CoT 的性能；**
4. **示例中的错误，或者无效的推理步骤不会导致 CoT 性能的下降；**

综上所述：CoT需要大模型具备一些方面的最基础的知识，如果模型过小，则会导致模型无法理解解决问题所需要的原子知识，CoT可以为所需用到的基础知识之间架起桥梁，使得大模型在回答问题期间不会跑偏。**最后，CoT 的作用，或许在于强迫模型进行推理，而非教会模型如何完成推理**，CoT 只是向模型指定了一种输出格式，规范模型让模型逐步生成答案。下图示例：

![img](https://pic1.zhimg.com/v2-b881ea42d8e545b4ad026495375f4ba2_r.jpg)

### CoT适合的场景

1. 使用大参数量模型
2. 任务需要复杂的推理：
3. 参数量的增加无法使模型性能得到显著提升

> CoT 更加适合复杂的推理任务，比如计算或编程，不太适用于简单的单项选择、序列标记等任务之中，并且 CoT 并不适用于那些参数量较小的模型（20B以下），在小模型中使用 CoT 非常有可能会造成机器幻觉等等问题。

![img](https://pica.zhimg.com/v2-121e0e621923bd6784a4d59c15c5962c_r.jpg)

而从理论角度，一篇来自斯坦福的论文《Why think step-by-step? reasoning emerges from the locality of experience》揭示了**当大模型的训练数据表现出了如上图中的变量的局部簇结构（Local Clusters of Variables）时，CoT 将会展现极好的效果**。而变量的局部簇主要指训练数据中变量之间有着强的相互作用，相互影响的关系。

当给予大模型的示例之间彼此之间互相区分并不相同时，也有助于提升 CoT 的性能。

同时，逻辑依据是否与问题相关，逻辑推理步骤的顺序也会显著影响 CoT 的性能

使用代码数据训练大模型，或者使用符合 CoT 格式的数据训练模型也有助于提升 CoT 的性能。

总结一下:

> **CoT 应当被用于 20B 以上参数规模的模型之中，并且模型的训练数据应当于任务问题相关且彼此相互有较强的联结。**

### CoT构造

* **人工构造CoT**：需要人类专家或者标注员针对特定任务，比如数学题或逻辑问题，写出详细的思考步骤，然后作为示例输入给模型。比如，一个数学应用题，人工写出如何分解问题、每一步的计算和逻辑，然后给出答案。这样模型在遇到类似问题时，可以模仿这种步骤生成答案。

* **自动构造CoT**：

  * **Zero-shot CoT** ：Zero-shot CoT是不给示例，直接通过提示激发模型生成推理链。比如，在问题后面加上“请一步步思考”，然后模型自己生成步骤和答案。

  * **Auto CoT** ：在Zero-shot生成的基础上，选择一些好的示例，然后用少样本的方式让模型生成更可靠的推理链。比如，先用Zero-shot让模型生成多个推理链，挑选正确或合理的那些，作为少样本示例，再输入给模型处理新问题。不过自动生成的质量可能不如人工，导致模型产生幻觉，即生成错误但看似合理的推理。

    Auto-CoT 分为两个阶段：（1）问题聚类，对任务数据集进行聚类（2）示例采样：从每个聚类中心中选择一个代表性问题使用 Zero-Shot-CoT 生成思维链作为示例。

### CoT的发展

对于指令生成问题，又可以分为手动指令生成与自动指令生成，显然简单的“Let's think step by step”就属于手动指令生成模式，此外，另一类的手动指令生成模式是 Plan-and-Solve 方法，其主要思想在于让模型制定一个将任务分为更小子任务的计划，再让模型一步一步执行计划、解决问题，其 Prompt 为“让我们首先了解问题并制定解决问题的计划。然后，开始执行计划，**逐步解决问题**”。

![img](https://pic1.zhimg.com/v2-a34a1935b44e819403a7689097a4c3c8_1440w.jpg)

显然，手动指令生成无法适应复杂的实际情况，因此自动指令生成应运而生，自动指令生成的代表作有两个，分别是自动 Prompt 工程（APE）以及提示优化（OPRO），**如上图所示，APE 与 OPRO 的核心思想都在于设计了一套机制让大模型通过观察各个候选的 Prompt 的实际任务中的表现，通过最大化表现得分来自动选择最优的 Prompt 。**

### CoT的推理结构

PoT，Tab-CoT，ToT 以及 GoT-Rationale

![img](https://pic2.zhimg.com/v2-26e83ec6134d28f8bac10de03379c3f1_r.jpg)

* **PoT** ：其中P指的是Programm程序，PoT的思想是，对思维链中可能会出错的一些计算问题，让大模型生成出编程语言然后在解释器中执行，将复杂运算和文本生成解耦。
* **Tab-CoT**：其中Tab指的是表格，在 ToT 中，研究者迫使大模型在每一步的推理中记录一个“∣步数∣子问题∣过程∣结果∣”的推理表格，并让大模型在推理时从生成的表格中提取答案，从而增强大模型的推理能力。
* **ToT**：其中 T 指 Tree 即思维树，将 CoT 的链式结构扩展为树形结构。ToT 让大模型在解决子问题时生成多个不同的答案选择，通过此建立的树形结构让大模型可以展望未来确定下一步的决策并且通过追溯来纠正历史决策。
* **GoT** ：其中G为graph，GoT 系统的核心在于一个“控制器”，控制器处理对图的操作（GoO）以及图状态推理（[GRS](https://zhida.zhihu.com/search?content_id=236778054&content_type=Article&match_order=1&q=GRS&zhida_source=entity)），其中 GoO 用于将一个给定的任务进行图分解，将一个任务分解为相互连接的节点-边关系，而 GRS 则负责维护大模型在 GoO 生成的图上的推理过程，记录当前步的状态，决策历史等等信息。

CoT 验证最经典的工作即是自我验证（Self-Verification），自我验证有两个步骤，分别是（1）对多个候选的推理路径进行采样；（2）给定问题结论让大模型验证条件是否满足结论，并根据验证分数对候选结论进行排序。

![img](https://picx.zhimg.com/v2-bb6c9ce7dd4bc48844b5ac21be1391bb_1440w.jpg)

而引入外部工具的 CoT 验证的代表性工作譬如 CRITIC 框架，**CRITIC 使得大模型可以交互式的引入外部工具来验证与修改自己的答案输出，经过大模型输出，外部工具验证，验证结果反馈，反馈修改四个循环的步骤加强 CoT 输出的可靠性**。而将 CRITIC 的思想进一步推向机制，即出现了任务自适应与流程自动化的 AuRoRA，AuRoRA 从多个来源提取相关知识，将不同来源的知识进行组合、检查与提炼来修改初始 CoT，以提示 CoT 的准确性与逻辑性。

在论文《Can large language models really improve by selfcritiquing their own plans?》中，作者质疑了**大模型是否可以真的进行可靠的 CoT 验证，在大模型的能力本身“无法解决验证结果反馈提出的问题”时，大模型有可能会过度纠正推理过程，直接跳过正确答案**。

### CoT应用场景

多模态 CoT 具有很大的应用前景，在 CoT 中，多模态可以分为两类：输入多模态与输出多模态。

![img](https://pic4.zhimg.com/v2-53ae2818fc4deec9dde7e1bddc30276f_1440w.jpg)

其中，MM-CoT 是输入多模态研究的第一篇工作，MM-CoT 侧重使用微调方法嵌入 CoT，通过将语言和图像合并在一个包含推理生成与答案推理的两阶段的框架中，使用微调大模型赋予输入多模态 CoT 的能力。基于 MM-CoT，GoT-Input 方法通过对 CoT 生成的思维图进行抽取构建三元组，并使用 [GNN](https://zhida.zhihu.com/search?content_id=236778054&content_type=Article&match_order=1&q=GNN&zhida_source=entity) 将文本、图像与 CoT 统一，从而生成包含 CoT 信息的最终答案。而区别于输入多模型，VCoT 解决了一个输出多模态的问题，VCoT 通过以生成图片的“标题”以及识别核心关注点作为图像生成的启动过程，通过递归的方式填充图像信息，从而实现输出多模态。

### 记忆CoT

**一般而言，大模型智能体通常同时拥有短期记忆与长期记忆的能力**。短期记忆一般作为一种时间信息，可以在 Agent 的多轮交互中灵活的改变（因此也被称为工作记忆），短期记忆为大模型提供更加直接的上下文信息支持，因此很自然的可以被建模为一条历史动作链。

相比于短期记忆的“动态性”，长期记忆更多的提供历史事件中的静态信息的记录，是对历史知识更加宏观与抽象的理解，长期记忆可以依赖于大模型中的可训练参数进行构建，也可以通过外部维护的记忆库进行构建。

**而当序列长度变长，线性链条式的记忆链效率出现下降时，为了实现针对“记忆”高效的增删改查，一些工作探索了树搜索与矢量检索的方法**。

其中，树搜索将记忆信息以树结构进行存储，让智能体通过迭代访问文本记忆信息，**譬如斯坦福 25 人小镇论文中提出的反思树 Reflection Tree，当智能体面对与环境的多轮交互时，反思树可以让智能体定期抽取历史信息进行“反思”，将反思抽象得到的结果搭建构成一颗反思树，树的叶子节点代表大模型每轮的基本观察，而非叶子节点则代表反思树的抽象程度，越靠近根节点抽象程度越高**。

而另一种方法则是矢量检索，**通过将复杂数据类型建模为矢量数据库来实现长期记忆的高效存储与检索**，当智能体遇到新问题需要“回忆”过往记忆时，基于矢量数据库的长期记忆系统则会快速检索相关信息，确保智能体行为一致性。

### 推理CoT

**借鉴 CoT 的思路让智能体分解任务逐步进行计划与决策以增强智能体解决问题的可靠性。**

AgentBench 强迫大模型智能体通过“思考”+“行动”步骤完成任务，而行动链技术通过一系列行动历史与未来行动计划帮助智能体进行决策，从而将决策问题转化为 CoT 推理问题。

### CoT的局限性

**首先**，**思维链必须在模型规模足够大时才能涌现**。

在 Jason Wei 等的研究中，PaLM 在扩展到 540B 参数时，与思维链提示结合，才表现出了先进的性能。一些小规模模型，思维链并没有太大的影响，能力提升也不会很大。

**其次**，**思维链的应用领域是有限的**。

目前，思维链只是在一些有限的领域，比如数学问题，五个常识推理基准（CommonsenseQA，StrategyQA，Date Understanding 和 Sports Understanding 以及 SayCan）上显现出作用，其他类型的任务，像是机器翻译，性能提升效果还有待评估。

**此外，即使有思维链提示，大语言模型依然不能解决小学水平的数学问题**。

没有思维链，数学推理是指定不行。但有了思维链，大语言模型也可能出现错误推理，尤其是非常简单的计算错误。Jason Wei 等的论文中，曾展示过在 GSM8K 的一个子集中，大语言模型出现了 8% 的计算错误，比如6 * 13 = 68（正确答案是78）。







