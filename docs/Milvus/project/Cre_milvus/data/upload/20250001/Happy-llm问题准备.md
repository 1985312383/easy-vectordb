# Happy-llm问题准备

## chapter1》NLP基础

---

### 1. 什么是自然语言处理（NLP）？它的主要应用场景有哪些？
**答案**：  
自然语言处理（NLP）是人工智能的一个分支，致力于让计算机能够理解、分析、生成和处理人类语言。主要应用场景包括机器翻译、文本分类、情感分析、对话系统、自动摘要、信息检索等。

### 1.1 什么是梯度

梯度是一个向量，包含了该函数关于每个输入变量的偏导数。具体来说，函数 ff在点 (x1,x2,...,xn) 的梯度记作 ∇f 或者 grad(f)，定义为：

$$∇f=(∂f∂x1,∂f∂x2,...,∂f∂xn)$$

这意味着梯度是一个向量，其中每个分量表示函数 f 在相应方向上的变化率。

### 中文分词是什么，为什么它在自然语言处理中是一个重要的任务？

中文分词是将连续的中文文本切分成有意义的词语序列的过程。因为中文不像英文那样通过空格来区分词与词之间的界限，所以需要使用特定的算法或模型来识别和分割词语，这对于后续的NLP任务（如文本分类、信息检索等）至关重要。

### 请解释一下子词切分，并给出一个例子。

子词切分是一种处理未登录词或复杂词汇的技术，它将词分解成更小的部分，这些部分能够描述原始词的意义。例如，“unhappiness”可以被切分为["un", "happy", "ness"]；同样地，“自然语言处理”可以被切分为["自", "然", "语", "言", "处", "理"]或者更合理的组合["自然", "语言", "处理"]。

###  什么是N-gram模型，以及它如何用于计算一句话的概率？

是一种用来预测一句话中下一个词出现概率的方法基于马尔可夫假设：

- 如果 N=1，叫 unigram（一元），只考虑单个词出现的概率。

- 如果 N=2，叫 bigram（二元），每个词的概率只和前面那个词有关。

- 如果 N=3，叫 trigram（三元），每个词的概率和前面两个词有关。

  假设有句子 The quick brown fox

- 用 trigram（三元模型）时，计算 brown 这个词的概率时，会看它前面两个词（The 和 quick），
  即 P(brown | The, quick)。

N-gram 模型会把整句话的概率分解成多个 N-gram 的概率相乘。比如，用 trigram 模型，句子的概率就等于：`P(第3个词 | 第1、2个词) × P(第4个词 | 第2、3个词) × ...`

但是：

- N 越大，所需的数据就越多，很多词组合其实很少见或者没出现过，导致模型学不到这些情况（数据稀疏问题）
- 忽略了更远距离的词之间的联系，不能理解复杂语义

### 尝试总结什么是Word2Vec

词嵌入技术，利用词在文本中的上下文信息来捕捉词之间的语义关系，是的语义相似的词在向量空间的距离较近，这样在召回的时候，可以使用k-mean或者HDBSCAN算法来聚类这些相似度接近的词语或者语句，然后返回给模型，作为问答的高质量参考。

​	Word2Vec模型主要有两种架构：连续词袋模型CBOW(Continuous Bag of Words)是根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示；Skip-Gram模型与CBOW模型相反, 是利用目标词的向量表示计算上下文中的词向量. 实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好。

​	Word2Vec模型能够捕捉到词与词之间的语义关系，比如”国王“和“王后”在向量空间中的位置会比较接近，因为在大量文本中，它们通常会出现在相似的上下文中。

---

### 3. 请说明大语言模型（如 GPT、BERT）与传统NLP模型的主要区别是什么？
**答案**：  
大语言模型采用大规模数据和深层神经网络，通过预训练-微调的方式获得更强的理解和生成能力。相比传统的统计模型或浅层神经网络模型，它们在语义理解、上下文建模等方面表现更好。



------------------

----------------



## chapter2》Transformer架构

### 1. Transformer 的核心思想是什么？它主要解决了什么问题？

**答案：**  
Transformer 的核心思想是利用自注意力（Self-Attention）机制来捕捉序列中任意位置的依赖关系，摆脱了 RNN 的顺序计算限制。这样可以并行处理序列数据，大幅提升训练效率，并且在长距离依赖建模上表现更好。

------

### 2. Transformer 的结构主要包括哪些部分？每个部分的作用是什么？

**答案：**  
Transformer 主要由编码器（Encoder）和解码器（Decoder）组成。编码器负责将输入序列编码为隐藏表示，解码器则根据编码器的输出生成目标序列。每个编码器和解码器都由多层堆叠，每层包含多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed Forward Network），并配有残差连接和层归一化。

### 什么是梯度消失、爆炸

- 梯度消失：在深层网络中，靠近输入层的权重由于梯度接近于0而难以得到有效的更新，是使得网络无法学习到有用的特征表示，尤其是那些自然语言处理（有上下文，长距离任务）
- 梯度爆炸：指相反的情况，即梯度变得异常大，导致权重更新幅度巨大，使得网络不稳定，甚至可能导致数值溢出错误。训练过程不稳定，网络性能下降。

------

### 3. 什么是多头自注意力机制？它为什么重要？

**答案：**  注意力机制在一段序列中，很难拟合全部的相关关系，所以有了多头注意力机制，即**对同一个语料进行多次注意力的计算**，每次注意力计算都能拟合不同的关系，将最后的多次结果拼接起来作为最后的输出，即可更全面深入地拟合语言信息。

​	事实上，所谓的多头注意力机制其实就是将原始的输入序列进行多组的自注意力处理；然后再将每一组得到的自注意力结果拼接起来，再通过一个线性层进行处理。这样可以捕捉到丰富的表达能力和不同层次的特征。

------

### 4. 为什么 Transformer 要加入位置编码？常见的位置编码方式有哪些？

**答案：**  
由于 Transformer 完全基于注意力机制，没有内建序列顺序信息，因此需要加入位置编码（Positional Encoding）来提供序列中每个位置的信息。常见方式有正弦-余弦位置编码（Sinusoidal Positional Encoding）和可学习的位置编码（Learnable Positional Encoding）。

- 多头注意力允许模型从不同的角度理解和处理输入数据。每个头可以专注于不同类型的关系或特征，从而提供更丰富和多样化的信息表示。
- 由于不同头可以关注序列中的不同部分，因此它们能够更好地捕捉长距离依赖以及复杂的语法和语义结构。
- 模型能够更精确地捕捉细微的区别和关联。
- 看起来计算量很多，但每个的注意力计算都是独立的，充分利用了GPU的并行能力

------

### 5. 请简述 Transformer 在自然语言处理中的优势和局限。

**答案：**  
优势：  

- 并行计算高效，适合大规模数据训练  
- 长距离依赖建模能力强  
- 在多种 NLP 任务（如翻译、文本生成）中表现优异

局限：  

- 对于超长文本，计算和内存消耗大  
- 需要大量数据和算力支持

------

### 6. 你如何理解残差连接和层归一化在 Transformer 中的作用？

**答案：**  随着神经网络层数的增加，传统的深层网络面临两个主要问题：**梯度消失/爆炸**和**退化问题**。这些问题限制了模型的深度和性能。

> 对于非常深的网络，如果每层的梯度都非常小或非常大，那么经过多层后，梯度可能会变得极小（梯度消失）或极大（梯度爆炸），从而导致训练不稳定甚至无法收敛。

> 即使解决了梯度消失/爆炸的问题，更深的网络并不总是表现得更好。在某些情况下，更深的网络可能比浅层网络表现更差。这是因为深层网络难以优化，即使有足够的训练时间，网络也可能陷入较差的局部最优解。

​	残差连接，即**下一层的输入不仅是上一层的输出，还包括上一层的输入**。

- 残差连接的出现，很有效的**保持了梯度的稳定性和强度，从而支持更深的网络训练** 。

归一化核心是为了**让不同层输入的取值范围或者分布能够比较一致**。由于深度神经网络中每一层的输入都是上一层的输出，因此**多层传递下，对网络中较高的层，之前的所有神经层的参数变化会导致其输入的分布发生较大的改变 **。各层的输出分布差异随着网络深度的增大而增大。但是，需要预测的条件分布始终是相同的，从而也就造成了预测的误差。**归一化操作可以帮助稳定每一层的输入分布，从而加速训练并提高模型的表现。** 

------

### 7. 你能用自己的话描述自注意力机制是如何工作的么？

**答案参考：**  注意力有三个核心变量：**Query**（查询值）、**Key**（键值）和 **Value**（真值），

​	假设有如下简化的新闻报道：“开营会议马上召开，时间为2025年6月9日。”

- Query可能是“时间在哪里？”
- Keys将是每个词（”开营“，“会议”，“马上”，“召开”，“时间”，“为”，“2025年”，“6月”，“9日”）的向量表示。
- Values也是这些词的向量表示。

key可以视为标识符，用于和query进行比较，确定哪些部分与query最相关。value就是后续的输出。通过计算Query与所有Key之间的相似度或相关性来确定对每个词的关注程度，结果是一组权重，反映了从Query出发，对文本中每一个token应该分配多少注意力，**然后，利用这些权重对相应的Values进行加权求和，从而获得最终的输出结果。这意味着，那些与Query高度相关的部分（如包含时间信息的词）将对最终结果产生更大的影响。**

Happy-llm提到了如何计算注意力分数，使用词向量，进行点积运算，首先对query进行向量化，然后对key中的某一个进行向量化，然后进行点积运算，获取一个相似度值，然后选中key的下一个，继续这样操作，计算 Query 和每一个键的相似程度。

然后我们通过一个Softmax层将其转化为和为1的权重$$ \text{softmax}(x)*i = \frac{e^{xi}}{\sum*{j}e^{x_j}} $$

**这样，得到的向量就能够反映 Query 和每一个 Key 的相似程度，同时又相加权重为 1，也就是我们的注意力分数了。**

不过，此时的值还是一个标量，同时，我们此次只查询了一个 Query。我们可以将同时一次性查询多个 Query，同样将多个 Query 对应的词向量堆叠在一起形成矩阵 Q，这个矩阵的每一行都是一个query：

> 对于这个Q矩阵，比如句子“I love NLP”有3个词，就会有3个Query，分别对应每个词的向量。我们可以把这些Query堆叠成一个矩阵 Q，一次性计算所有Query对应的注意力结果。这样就
>
> - 不需要一个一个地单独计算每个Query，节省大量时间。
> - 充分利用GPU的并行计算能力。
>
> 并且！模型可以关注到整个句子的各个词的相关性，对于长难句或者长距离依赖的处理更加好。

1. 对于每个查询，计算它与所有Key之间的相似度。这可以通过矩阵乘法实现，其结果是一个矩阵，其中每个元素表示对应Query和Key之间的相似度。
2. **权重转换**：同样地，我们需要使用Softmax函数将这些相似度转换为权重。这里的Softmax是对每个查询分别应用的，以确保每组权重加起来等于1。
3. **加权求和**：最后，我们利用这些权重对Values进行加权求和，得到针对每个查询的输出。这一步也是通过矩阵乘法完成的。



### 7.1 自注意力机制有什么好处呢，是如何实现这些“好处”呢

比如在语言处理场景下，你要把英文句子 “The cat is on the mat.” 翻译成中文，需要知道每个中文词应该“关注”英文句子中的哪个部分。比如，猫这个字就需要关注英文语句中的cat，垫子这个词就要关注mat。注意力机制就像让模型在处理一个目标词时，自动选择性关注源语言中最相关的部分。

自注意力机制的关键在于认识到它允许每个位置的token能够直接与其他所有位置的token进行交互，这与传统的RNN或者LSTM不同，RNN只能顺序的处理，浪费了GPU并行计算的能力。

- 提高了模型处理长距离依赖的关系
- 使得模型结构更加适合并行化处理，提高了训练效率

### 8.为什么我们要用矩阵形式的注意力？

优势1：并行化处理多个查询（Query）

在Transformer等模型中，输入是一个序列（比如一句话），每一个词都会成为一个Query。

例如，句子“I love NLP”有3个词，就会有3个Query，分别对应每个词的向量。我们可以把这些Query堆叠成一个矩阵 [QQQ]，一次性计算所有Query对应的注意力结果。

✅ **好处**：

- 不需要一个一个地单独计算每个Query，节省大量时间。
- 充分利用GPU的并行计算能力。

📈 优势2：捕捉全局依赖关系

传统的RNN只能按顺序处理信息，前面的词很难影响后面的词（特别是距离很远的时候）。而注意力机制允许每个Query同时看到所有的Key，也就是说：

> 每个词都可以直接“看到”整个句子中所有其他词的相关性。

✅ **好处**：

- 更容易捕捉长距离依赖（比如句首和句尾之间的联系）。
- 对复杂语义结构理解更好。

⚡ 优势3：提升模型表达能力

通过不同的Query、Key、Value组合，注意力机制可以灵活地建模各种类型的关系：

- Query 和 Key 相似 → 高注意力分数
- Query 和 Key 不相关 → 低注意力分数

这使得模型可以根据上下文自动调整关注点，而不是固定地只看前几个词或者后几个词。

-----

-----



## chapter3》预训练模型

### 什么是双向LSTM，与传统的LSTM的区别是什么

**双向LSTM：**是一种特殊的递归神经网络RNN变体，通过同时考虑时间序列数据的前后信息来增强模型的表现，传统的LSTM只能从一个方向处理序列数据（从前向后），这意味着它们在任一时刻的预测只能基于该时刻之前的信息。但在实际情况下，理解某个词的意思往往需要同时参考其给出的上下文信息。

### 为什么传统的预训练任务都是LM

LM 使用上文预测下文的方式可以直接应用到任何文本中，对于任意文本，我们只需要将下文遮蔽将上文输入模型要求其预测就可以实现 LM 训练，因此互联网上所有文本语料都可以被用于预训练。

### **什么是 MLM（Masked Language Model）？它与传统的 LM（Language Model）有何不同？**

- **MLM** 是一种预训练任务，其工作原理是在文本序列中随机遮蔽部分 token，并要求模型根据上下文预测这些被遮蔽的 token。这与传统的 **LM** 不同，后者通常是从左到右或从右到左地预测下一个词，仅依赖于单向的上下文。因此，MLM 能够利用双向的上下文信息来更好地理解语言结构和语义关系。

### **为什么说 MLM 能够更好地捕捉双向语义关系？请举例说明。**

- 由于 MLM 在预测被遮蔽的 token 时会同时考虑其左侧和右侧的上下文信息，这使得模型能够学习到更加全面和准确的语义表示。例如，在句子“I <MASK> you because you are <MASK>”中，模型需要结合“you because you are”以及“I”来推测出最可能的单词，如“love”和“wonderful”，这种能力是单向的语言模型难以达到的。

### **在 BERT 的 MLM 训练过程中，为何要对选定的 token 实施 80% <MASK>、10% 随机替换、10% 保持不变的策略？这种做法解决了什么问题？**

- 这种策略旨在减少预训练和微调之间的不一致性。10%的概率保持 token 不变是为了让模型适应没有 <MASK> 标记的真实场景；而 10% 的概率用随机 token 替换目标 token 则是为了确保模型不能简单地依赖于识别 <MASK> 标签来进行预测，而是必须持续关注整个输入序列的上下文信息，从而提高模型的泛化能力和对上下文的理解能力。

### **比较 BiLSTM 和基于 Transformer 的模型（如 BERT）在处理文本时的方法差异，并讨论它们各自的优势。**

- **BiLSTM** 通过两个方向（前向和后向）的 LSTM 来捕获序列中的信息，允许每个位置都考虑到整个序列的信息。然而，它的计算效率较低，尤其是在长序列上。相比之下，**Transformer** 模型（如 BERT）使用自注意力机制，可以在一次操作中并行处理所有位置的信息，极大地提高了计算效率和捕捉长期依赖的能力。此外，Transformer 可以更灵活地调整对不同位置的关注度，这是 BiLSTM 较难实现的。

### **考虑到 MLM 预训练任务的特性，您认为它更适合解决哪些类型的自然语言处理任务？请给出理由。**

- MLM 特别适合那些需要深入理解文本语义的任务，比如问答系统、情感分析、命名实体识别等。因为 MLM 强调了对文本深层次语义的理解，而不是简单的模式匹配或词汇级的预测。通过预训练，模型学会了如何利用丰富的上下文信息进行推理，这对于许多复杂的 NLP 应用来说是非常有价值的。