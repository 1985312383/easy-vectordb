# Meta-Chunking

文本分块在 RAG 中起着关键作用，低效的分块策略会导致上下文不完整或包含过多无关信息，从而损害问答系统(QA)的性能(yu2023chain)。

在RAG的流程中，文本分块往往是容易被忽视的关键环节，这篇论文中提出Meta-Chunking这一元分块框架，**通过识别最优分割点与保留全局信息的双重策略，专门提升分块质量**。首先突破基于相似度的分块局限，我们利用LLM的能力，设计了两种基于不确定性的自适应分块技术**Perplexity Chunking and Margin Sampling Chunking**针对不同文本的固有复杂性，我们通过动态合并实现元分块，在细粒度与粗粒度文本分块间取得平衡。此外，我们建立了全局信息补偿机制，包含两阶段层次化摘要生成流程与聚焦缺失反思、精修和补全的三阶段文本块重写过程。这些组件共同增强了文本块的语义完整性与上下文连贯性。



- Through lightweight chunking algorithm design, the logical analysis capability of LLMs is decoupled into computable the PPL features and MSP indicators, achieving identification of textual logical boundaries and dynamic balance of chunking granularity.

  • 通过轻量级分块算法设计，将 LLMs 的逻辑分析能力解耦为可计算的 PPL 特征和 MSP 指标，实现文本逻辑边界的识别与分块粒度的动态平衡。

- We establish a information compensation mechanism that collaboratively executes through a three-stage missing-aware rewriting process and a two-stage context-aware summary generation, repairing the semantic discontinuities in text chunks.

  • 我们建立了信息补偿机制，通过三阶段缺失感知重写流程与两阶段上下文感知摘要生成的协同执行，修复文本块中的语义断裂问题。

> 1. **PPL (Perplexity)**：困惑度是自然语言处理中一个常用的评价指标，用于衡量一个概率模型预测样本的能力。简单来说，它评估了语言模型在预测下一个词时的不确定性。较低的困惑度意味着模型对文本的理解更好，能够更准确地预测下一个词。因此，在轻量级分块算法设计中，PPL 特征可能被用来作为衡量文本逻辑连贯性的一个标准，以帮助识别文本的逻辑边界。
> 2. **MSP (Most Significant Passage)**：虽然“MSP”这个缩写在特定领域可能有多种含义，但在这里，我们可以假设它指的是“最重要段落”或“最具代表性片段”。在给定的语境下，MSP 指标可能是用于识别文本中最能代表其主要内容或主题的部分。通过计算每个分块的重要性或代表性，可以帮助实现分块粒度的动态平衡，确保每个分块既不过于细碎也不过于笼统，从而更好地保持原始文本的逻辑结构和信息完整性。

我们的方法基于一个核心理念：通过允许分块大小的可变性，更有效地捕捉并保持内容的逻辑完整性。这种动态调整的粒度控制确保每个分割后的块都包含完整且独立的思想表达，从而避免分割过程中出现逻辑链条断裂。这不仅提升了文档检索的相关性，同时也增强了内容的清晰度。

如图 1 所示，我们的方法融合了传统文本分块策略的优势，例如遵循预设的分块长度约束和确保句子结构完整性，同时增强了在分块过程中保证逻辑连贯性的能力。我们将通过分块获得的每个文本块称为元块（Meta-Chunk），它由段落中按顺序排列的句子集合组成。这些句子不仅具有语义相关性，更重要的是还包含深层次的语言逻辑联系，包括但不限于总分、并列、递进和例证等关系，如图 4 所示。通过观察发现，元块内部连续句子之间往往存在紧密的逻辑联系，但由于内容表征的差异性，这些句子会表现出较低的语义相似度。正如 qu2024semantic 所述，语义分块在多个实验范式中均未能展现出优势。 我们认为这一现象与语义相似度算法的原始理论建模意图密切相关。这些方法本质上是通过建模文本间的语义重叠程度，来量化两个段落之间或句子与段落之间的关联性。然而在微观层面，那些具有逻辑关联却表达不同内容的句子限制了其适用性。详细分析见附录 B。为解决上述问题，我们基于 LLMs 中的不确定性理论实施了以下策略。



![Refer to caption](https://arxiv.org/html/2410.12788v3/extracted/6463559/pic/kuangjia.png)

**核心目标：** 将长文本分割成多个**连贯**的语块 (`X1, X2, ..., Xk`)，每个语块由连续的句子组成 (`xi`)，并且最终语块的长度尽可能满足用户需求。
$$
\text{PPL}_M(x_i) = \sum_{k=1}^{K} \frac{\text{PPL}_M(t_k^i \,|\, t_{<k}^i, t_{<i})}{K}
$$
**整体流程详解：**

1.  **句子分割 (Sentence Segmentation):**
    *   输入：原始长文本。
    *   处理：使用句子分割工具（如基于规则、NLP库NLTK/spaCy等）将文本分割成一个个**完整的句子**。
    *   输出：句子序列 `S = (x1, x2, ..., xn)`。其中 `xi` 代表第 `i` 个句子。`n` 是总句子数。

2.  **逐句计算 PPL (Perplexity Calculation per Sentence):**
    *   **目标：** 量化当前句子 `xi` 在给定其**之前所有文本内容** (`x1, x2, ..., xi-1`) 的上下文下，语言模型 (LLM `M`) 对其的“惊讶程度”或“预测难度”。
    *   **PPL 定义 (回顾):** 困惑度 (Perplexity, PPL) 是衡量语言模型预测能力的一个指标。**PPL 越低，表示模型对当前词序列的出现越不感到“困惑”，即该序列在给定上下文中越“自然”、越“可预测”、越“连贯”。**
    *   **公式 (1) 详解：**
        `PPL_M(xi) = ( Σ_{k=1}^{K} PPL_M(t_k^i | t_{<k}^i, t_{<i}) ) / K`

        *   `xi`: 当前要计算 PPL 的第 `i` 个句子。
        *   `K`: 句子 `xi` 中包含的 **token (词元)** 的总数量。例如，`xi` = “我喜欢苹果” 可能被分词成 `["我", "喜欢", "苹果"]`，那么 `K=3`。
        *   `t_k^i`: 句子 `xi` 中的第 `k` 个 token。
        *   `t_{<k}^i`: 句子 `xi` 中，排在 `t_k^i` **之前**的所有 token。即 `xi` 句子内部，`t_k^i` 的上文。
        *   `t_{<i}`: 所有排在句子 `xi` **之前**的 token。即整个文档中，`xi` 之前的所有句子的所有 token。这是 `xi` 的**外部上文**，提供了最重要的语境。
        *   `PPL_M(t_k^i | t_{<k}^i, t_{<i})`: **核心条件概率计算。** 这表示在给定以下两部分的条件下：
            *   `t_{<k}^i` (句子 `xi` 内部，`t_k^i` 之前的 token)
            *   `t_{<i}` (整个文档中，`xi` 之前的所有 token)
              语言模型 `M` 预测出 token `t_k^i` 的**困惑度**。更基础的计算通常是模型预测 `t_k^i` 的**概率** `P_M(t_k^i | t_{<k}^i, t_{<i})`。PPL 是概率的衍生度量（通常基于概率的对数和平均）。
        *   **`Σ_{k=1}^{K} ... / K`**: 对句子 `xi` 中的 **每一个 token (`k` 从 1 到 `K`)** 计算其条件 PPL，然后将这 `K` 个值**求和后再除以 `K` (句子 token 总数)**。这得到了句子 `xi` 在整个上下文 (`t_{<i}`) 下的**平均 token 级困惑度**。**这个平均值 `PPL_M(xi)` 就代表了整个句子 `xi` 在给定前文 (`t_{<i}`) 下的连贯性/自然度分数。分数越低，句子 `xi` 与前文的衔接越流畅、越符合预期。**

3.  **构建 PPL 序列 (PPL Sequence Construction):**
    *   处理：对序列 `S` 中的**每一个句子 `xi` (i 从 1 到 n)** 都执行步骤 2，计算其 `PPL_M(xi)`。
    *   输出：得到一个与句子序列对应的 PPL 值序列：`PPL_seq = (PPL_M(x1), PPL_M(x2), ..., PPL_M(xn))`。
    *   **关键洞察：** 当一个句子 `xi` 与其前文 (`t_{<i}`) 在语义或话题上发生**转换或断裂**时，模型预测 `xi` 的难度会**增大**，导致 `PPL_M(xi)` **升高**。相反，如果 `xi` 与前文紧密相关，则 `PPL_M(xi)` 会相对**较低**。更重要的是，在**话题结束点或新话题开始点**附近，我们预期会看到 `PPL_seq` 中的**局部极小值 (local minima)**。这些极小值点标识了文本中相对**更自然、更连贯的潜在分割位置**。

4.  **识别关键分割点 (Finding Segmentation Points - Minima Detection):**
    *   目标：分析 `PPL_seq`，找出可以作为潜在语块边界的点（主要是局部极小值点）。
    *   **算法关注两类极小值点 (对于点 `i`, 即句子 `xi` 的位置)：**
        *   **类型 1 (波谷):** `PPL_M(xi-1) > PPL_M(xi)` **且** `PPL_M(xi+1) > PPL_M(xi)` **且** (`PPL_M(xi-1) - PPL_M(xi) > θ` **或** `PPL_M(xi+1) - PPL_M(xi) > θ`)。
            *   解释：点 `i` 的 PPL 比它左边 (`i-1`) 和右边 (`i+1`) 邻居的 PPL 都低（形成一个谷底），并且**至少有一侧**的下降幅度（左邻居减当前值 或 右邻居减当前值）超过了预设的**阈值 `θ`**。`θ` 是一个可调参数，用于过滤掉微小的、可能由噪声引起的波动，只关注显著的连贯性转换点。
        *   **类型 2 (左悬崖):** `PPL_M(xi-1) - PPL_M(xi) > θ` **且** `PPL_M(xi+1) == PPL_M(xi)`。
            *   解释：点 `i` 的 PPL 相比其**左邻居 (`i-1`)** 有一个**显著的下降**（下降幅度 > `θ`），而其**右邻居 (`i+1`)** 的 PPL 与它**持平**。这通常表示一个话题在 `xi` 处结束或发生剧变，而 `xi` 和 `xi+1` 可能属于同一个新单元的开头。
    *   输出：一个潜在分割点位置的集合 `B = {b1, b2, ..., bm}` (例如 `b1=3`, `b2=7`, `b3=12` 表示建议在句子3后、7后、12后进行分割)。这些点标识了文本中相对连贯的子单元（元块）之间的边界。

5.  **生成元块 (Meta-Chunk Formation):**
    *   处理：根据分割点集合 `B` 将句子序列 `S` 切分开。
    *   输出：元块序列 `MC = (MC1, MC2, ..., MCp)`。
        *   每个元块 `MCj` 由**两个相邻分割点之间（或从开头到第一个分割点，或最后一个分割点到结尾）的所有连续句子**组成。
        *   例如，分割点 `B = {3, 7, 12}`, 句子总数 `n=15`，则：
            *   `MC1 = (x1, x2, x3)` (句子1到句子3)
            *   `MC2 = (x4, x5, x6, x7)` (句子4到句子7)
            *   `MC3 = (x8, x9, x10, x11, x12)` (句子8到句子12)
            *   `MC4 = (x13, x14, x15)` (句子13到句子15)
        *   **元块 `MCj` 的特性：** 算法认为每个元块内部的句子在给定的上下文（前文）下，具有相对较高的连贯性（由 PPL 极小值点标识其边界）。它们是**基于文本自身语义和结构**划分出的**基础连贯单元**。**元块的长度是算法根据连贯性自动决定的，通常是不均匀的。**

6.  **动态组合元块以满足长度需求 (Dynamic Combination for Final Chunks):**
    *   **目标：** 用户通常对最终输出语块 (`X`) 有**最大长度限制 `L`** (例如，不超过 512 tokens)。元块 `MCj` 本身可能太短或太长。这一步将**连续的元块合并**，形成最终的语块 `Xk`，使得每个 `Xk` 的 token 总数尽可能接近但不超过 `L`。
    *   **处理 (贪心策略示例):**
        *   初始化：从第一个元块 `MC1` 开始。
        *   当前合并组：`CurrentChunk = [MC1]`, `CurrentLength = len(MC1)` (token 数)。
        *   遍历后续元块 (`MC2`, `MC3`, ... `MCp`)：
            *   如果 `CurrentLength + len(MC_next)` <= `L`：
                *   将 `MC_next` 加入 `CurrentChunk`。
                *   `CurrentLength = CurrentLength + len(MC_next)`。
            *   否则 (`CurrentLength + len(MC_next)` > `L`)：
                *   将当前的 `CurrentChunk` 输出为一个最终语块 `Xk`。
                *   重置 `CurrentChunk = [MC_next]`。
                *   重置 `CurrentLength = len(MC_next)`。
        *   处理完所有元块后，将剩余的 `CurrentChunk` 输出为最后一个 `Xk`。
    *   **输出：** 最终语块序列 `X = (X1, X2, ..., Xk)`。
        *   每个 `Xk` 由一个或多个**连续的元块 (`MCj`)** 组成。
        *   每个 `Xk` 的总 token 长度 <= `L` (用户设定的最大长度)。
        *   **关键：** 组合发生在**元块的边界处**。算法**不会**在一个元块 `MCj` 内部（即其连贯句子组内部）进行切割。**优先保证了最终语块 `Xk` 内部包含的是完整的、算法认为连贯的子单元 (元块)。** 牺牲的是最终块可能由多个元块组成，但避免了破坏基础的连贯单元。

7.  **处理超长文本：KV 缓存机制 (Handling Very Long Texts: KV Cache):**
    *   **问题：** 计算 `PPL_M(xi)` 需要模型记住和处理 `xi` 之前的所有 token (`t_{<i}`)。对于超长文本，这会超过 LLM 的最大上下文长度或 GPU 内存限制。
    *   **解决方案：** 采用 **Key-Value (KV) 缓存** 并实施**滚动缓存策略**。
        *   **KV 缓存：** Transformer 模型在计算注意力时会生成 Key (`K`) 和 Value (`V`) 矩阵，代表之前 token 的信息。缓存这些 `K` 和 `V` 可以避免在预测后续 token 时重新计算前面的所有层，大大加速自回归生成和 PPL 计算。
        *   **滚动缓存策略 (Rolling Cache Strategy):**
            *   将整个文本按 token 分成多个**子序列** (Segment)。
            *   计算第一个子序列的 PPL 时，正常建立并保留其完整的 KV 缓存。
            *   当处理到后续子序列，并且**累计的 KV 缓存大小接近设备 (GPU) 内存上限或模型最大上下文长度**时：
                *   策略性地**丢弃最早的一部分子序列的 KV 缓存** (例如，丢弃最早 30% 的缓存)。
                *   保留最近、最相关的部分子序列的 KV 缓存 (例如，保留最近 70% 的缓存)。
            *   用剩余缓存 + 新 token 继续计算后续句子的 PPL。
    *   **权衡：** 丢弃部分历史缓存会损失一些远距离的上下文信息，可能轻微影响后续句子 PPL 计算的绝对准确性。但这种方法**在内存限制和保持足够连贯性上下文之间取得了平衡**，使得处理超长文本成为可能。通常，近期保留的上下文对于保持局部连贯性（即识别局部极小值点）已经足够重要。

**图 1 (Meta-Chunking Overview) 再解释：**

*   **圆圈 (Circles):** 明确代表句子 `x1, x2, ..., xn`。大小不一表示句子长度不同。
*   **垂直线 (Vertical Lines):** 位于某些句子之间，代表步骤 4 中通过分析 `PPL_seq` 找到的**潜在分割点**（极小值点）。它们是元块 (`MCj`) 的边界。
*   **相同背景色区域 (Same Background Color):** 覆盖一组连续的句子。**每个这样的区域就是一个元块 (`MCj`)**。例如，黄色背景覆盖句子 `x1, x2, x3` 构成了元块 `MC1`；蓝色背景覆盖 `x4, x5, x6, x7` 构成了元块 `MC2`；绿色背景覆盖 `x8, x9, x10` 构成了元块 `MC3`。
*   **"Dynamically combine meta chunks" 箭头:** 表示步骤 6 的过程。图中可能显示：
    *   元块 `MC1` (黄色) 被单独作为一个最终语块 `X1`（假设其长度已接近 `L`）。
    *   元块 `MC2` (蓝色) 和 `MC3` (绿色) 被**组合**起来形成最终语块 `X2`（因为单独 `MC2` 或 `MC3` 可能太短，组合后总长度接近 `L`）。
    *   （图中未明确画出但逻辑包含）后续的元块（如 `MC4`）可能被组合或单独形成 `X3`, `X4` 等。
*   **最终输出 (Final Output):** 最终提供给用户的是语块 `X1, X2, ...`，其边界就是组合后的元块边界（图中背景色的交界处）。`X1` 对应 `MC1` (黄色区域), `X2` 对应 `MC2+MC3` (蓝色+绿色区域)。

我们主要关注两类极小值点：当某点两侧的 PPL 均高于该点且至少一侧差值超过预设阈值 θ 时；或当左侧点与该点差值大于 θ 且右侧点等于该点值时。这些极小值被视为潜在语块边界。


若文本超出 LLMs 或设备的处理范围，我们会策略性地引入键值（KV）缓存机制：先按 token 将文本分割为若干部分形成多个子序列，随着 PPL 计算的推进，当 GPU 内存即将超过服务器配置或 LLMs 最大上下文长度时，算法会适时移除先前部分文本的 KV 键值对，从而不过度牺牲上下文连贯性。


我们来看上图中的左侧部分图片，（右侧是PPL的计算）

## 边缘采样分块（Margin Sampling Chunking）
首先，为什么要提出边缘采样分块呢？
### 技术背景与解决的问题
LumberChunker模型在处理长文本时，会首先将长文本切分为多个元块（Meta Chunks），然后对元块进行分块（Chunking），最后将分块后的结果进行拼接（Concatenation）得到最终的语块（Sentence）。
1. 依赖于特定格式文本生成
2. 需要正则表达式提取分块结果
3. 在小模型（<7B参数）上性能下降明显

MSP策略直接分析模型输出的概率分布，无需格式约束，计算复杂度降低，适合场景资源受限的场景，与PPL分块形成互补，覆盖不同类型文本边界，下面我们直接来看公式：

$$\text{Margin}_M(x_i) = P_M(y=k_1 | \text{Prompt}(x_i, X')) - P_M(y=k_2 | \text{Prompt}(x_i, X'))$$

随后，通过将 $\text{Margin}_M(x_i)$ 与阈值 $\theta$ 进行对比，就能得出这两个句子是否应被分割的结论。
![pic](../../src/MSP.jpg)
此外，为决策标准设定阈值是所有策略的普遍要求，为此我们引入了动态阈值机制。具体而言：
- 在 $\theta$ 的初始化阶段，我们为其赋予 0 的初始值。
- 随后，我们通过追踪历史 $\text{Margin}_M(x_i)$ 值并计算其平均值来微调 $\theta$，从而实现更灵活的分块调整。


单纯通过调整阈值控制分块大小可能导致随着阈值增大出现分块尺寸不均的问题


为此，我们提出元分块与动态合并相结合的策略，旨在灵活应对不同分块要求。

首先，采用 PPL 分块或 MSP 分块将文档划分为若干元分块，记作 $(c_1, c_2, \ldots, c_{\alpha})$。传统分块方法将句子视为独立逻辑单元，而我们则将元分块作为独立逻辑单元。元分块是基于文本自身语义和结构划分出的基础连贯单元，内部句子具有相对较高的连贯性。

随后，根据用户指定的分块长度 $L$，迭代合并相邻元分块直至总长度满足或接近要求。具体而言，我们会按顺序尝试合并相邻的元分块。若当前合并的元分块总长度刚好等于 $L$，或者当前合并的元分块总长度小于 $L$，但再加入下一个元分块后总长度就会超过 $L$，则将当前合并的这些元分块视为一个完整分块。例如，若满足 $\text{len}(c_1, c_2, c_3) = L$ 或 $\text{len}(c_1, c_2, c_3) < L$ 且同时满足 $\text{len}(c_1, c_2, c_3, c_4) > L$，则将 $c_1, c_2, c_3$ 视为完整分块。

为解决文本分块过程中因上下文信息丢失导致的语义鸿沟问题，我们提出了一种全局增强的文本重写与摘要生成机制。具体而言，我们利用 LLM 作为判别器来检测每个分块是否存在语义缺失，若存在则触发 3.2.1 节所述的重写流程。处理完这些缺失后，我们在 3.2.2 节对所有分块执行摘要生成以进一步提升召回率，为最终提升问答性能奠定坚实基础。详细设计方案详见附录 C。

### 3.2.1 全局增强型文本分块重写

#### 预处理（可选）
当文档太长，大语言模型（LLM）没办法完整处理时，我们可以用基于语义嵌入的跨片段关联性分析。简单来说，就是用一个能计算语义相似度的模型，把每个文本片段变成向量，再计算这些向量之间的余弦相似度，这样就能知道不同片段之间的语义关联有多强啦。通过这个方法，我们能找到和当前片段相关的上下文信息。

#### 第一阶段：缺失内容识别
借助大语言模型（LLM），再结合预处理阶段找到的相关信息，对每个文本片段进行深入分析。这一步的主要任务是找出当前片段里缺失的前提条件、背景知识、相关事实或者结论。大语言模型要把缺失信息的地方都列出来，还得说清楚需要补充什么内容。

#### 第二阶段：缺失信息精炼
在这一阶段，我们会对前一步检测出来的可能缺失的信息进行打分和筛选。这么做是为了避免添加一些无关或者错误的内容，保证补充信息的准确性。

#### 第三阶段：缺失信息补全
根据上一阶段确定的缺失位置和需要补充的信息，让大语言模型（LLM）把这些信息和当前的文本块整合在一起。最终目标是生成一个上下文连贯、读起来自然的新文本块，让不同文本块之间的信息能很好地融合。

### 3.2.2 上下文感知摘要生成
这部分的主要目的是为每个文本块生成简洁但包含全局信息的摘要，让文本块能更好地感知上下文。

#### 生成补充摘要
模型会利用全局信息，为目标文本块生成一个补充摘要。这么做是为了弥补文本块在分割过程中可能丢失的语篇背景和外部关联信息。

#### 生成局部摘要并融合
模型会单独对文本块的内容进行概括，生成一个总结核心观点的局部摘要。然后把这个局部摘要和前面生成的补充摘要融合提炼，得到一个能从全局角度介绍文本块内容的增强版摘要。

为了让我们提出的改写和摘要生成功能更好用，我们按照上面说的流程，分别为这两个功能构建了 20,000 个训练数据样本。同时，我们选择对小型语言模型（SLM）的所有参数进行微调。对于输入序列 $X$ 和目标输出序列 $Y=(y_1, y_2, \ldots, y_T)$，损失函数定义如下：

$L = -\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T}\log P(y_t|y_{<t}, X;\theta)$

其中 $P(y_t|y_{<t}, X;\theta)$ 表示模型在给定输入 $X$ 和之前生成的内容 $y_{<t}$ 时，预测出真实目标标记 $y_t$ 的概率，$\theta$ 表示模型的参数，$N$ 表示一批次里的样本数量。关于数据集构建和微调时超参数的具体配置，可以查看附录 C。

## 附录 C 语义补全详细流程

### C.1 语义补全的必要性
当原文被分割为孤立文本块时，每个文本块可能丢失跨块上下文关联、全局结构连贯性或隐含逻辑关系，从而引发以下问题：

- **信息不完整**：关键细节被截断或分散在多个文本块中（例如：某公式定义在块A，而公式应用却在块C）
- **语义不连贯**：文本块间的逻辑关系断裂（例如：原因陈述与结果分析被分割在不同块中）
- **噪声干扰**：无关内容被错误地包含在文本块中（例如：将举例说明与核心论点混合）

通过采用全局增强的重写与摘要生成技术，我们能够为每个文本块补充缺失的全局信息，弥合语义断层，最终提升RAG系统的响应质量。

### C.2 语义补全实施步骤
#### C.2.1 信息缺口识别阶段
我们首先采用QwQ-32B模型的长推理模式（可处理超长文本的特殊模式），全面识别以下类型的信息缺口：
- 明确引用但未定义的术语
- 需要前文信息才能理解的代词指代
- 跨章节的逻辑依赖关系
- 分散在不同位置的相关数据

#### C.2.2 补充信息过滤阶段
使用ERNIE-3.5-128K模型对潜在补充内容进行评分与过滤，具体指标包括：
- 相关性（与当前文本块主题的关联程度）
- 必要性（缺失后是否影响理解）
- 简洁性（补充内容是否精炼）

#### C.2.3 文本块融合阶段
将精炼后的信息片段与当前文本块内容融合，生成既保持上下文连贯性又具备更高语义完整性的文本段落。融合策略包括：
- 前置补充：对需要背景知识的文本块，在开头补充必要上下文
- 插入补充：对术语定义类缺口，在首次出现位置插入解释
- 后置补充：对结果分析类文本块，在结尾补充相关结论

### C.3 增强型摘要生成
ERNIE-3.5-128K模型采用两阶段策略生成增强型摘要：
1. **全局补充摘要**：利用文档级信息为当前文本块生成补充说明
2. **局部核心摘要**：提炼当前文本块自身的核心内容
3. **融合优化**：将两类摘要精细融合，形成既能体现局部重点又包含全局关联的增强型摘要

### C.4 训练数据构建
通过LLM驱动的数据蒸馏管道，我们构建了高质量训练样本集：
- 为语义补全模块构建20,000条实例
- 为摘要生成模块构建20,000条实例
- 所有样本均包含人工标注的质量评分

这些数据为小语言模型(SLM)的全参数微调提供了关键指导信号，使我们的框架能够在高性能与轻量级部署之间取得平衡。

### C.5 效果验证
![PPL分布变化趋势](https://arxiv.org/html/2410.12788v3/extracted/6463559/pic/ppl_rewrite.jpg)
**图5**：不同LLMs间原始文本块与改写文本块的PPL分布变化趋势

如图5所示，经过语义补全处理后（紫色线），各类模型的PPL值（困惑度）普遍低于原始文本块（橙色线），表明文本块的语义连贯性和可理解性得到显著提升。特别是在Qwen2-7B和Qwen2.5-7B模型上，PPL值降低了约30%，验证了语义补全策略的有效性。