{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# milvus 数据进行HDBSCAN聚类\n",
    "\n",
    "## 前言\n",
    "1. 本章的内容参考自[Milvus官方文档](https://milvus.io/cn/docs/v2.0.0/hdbscan_clustering.md),对其进行补充和修改。扩展内容请跳转[]()。\n",
    "2. 在本章节，建议使用本地docker 部署 Milvus 数据库，使用Attu查看数据是否插入成功。\n",
    "3. 建议手动去魔搭社区下载模型\n",
    "4. 笔者测试使用的python版本为3.12,下面为测试时所用的依赖包版本，请根据实际需求进行安装\n",
    "    ```\n",
    "        hdbscan               0.8.40\n",
    "        modelscope            1.27.1\n",
    "        numpy                 2.2.6\n",
    "        pandas                2.3.0\n",
    "        pip                   24.0\n",
    "        sentence-transformers 5.0.0\n",
    "    ```\n",
    "\n",
    "### 什么是HDBSCAN\n",
    "\n",
    "HDBSCAN 是一种基于密度的聚类算法，它能够处理噪声和任意形状的聚类。该算法通过计算数据点之间的距离来确定聚类，并能够自动确定聚类的数量。\n",
    "\n",
    "我们使用 BGE-M3 嵌入模型从新闻标题数据集中提取嵌入，利用 Milvus 计算嵌入之间的距离以帮助 HDBSCAN 进行聚类，然后使用 UMAP 方法将结果可视化以进行分析。\n",
    "\n",
    "### 为什么要聚类\n",
    "\n",
    "想象一下你面对一堆没有任何标签、杂乱无章的数据点（比如客户信息、用户行为日志、新闻文章、图片特征等）。你无法直接知道它们的内在结构。聚类就是为了解决这个问题而生的。它的核心目的是：\n",
    "*   **发现数据内在结构：** 自动地将数据集中**相似的对象**分组（聚在一起），形成不同的簇（Cluster）。相似的样本在同一个簇内，不相似的样本分到不同的簇。\n",
    "*   **数据简化与摘要：** 将大量的、复杂的数据压缩成相对较少的、有代表性的簇，便于人类理解和后续处理。用几个“典型代表”（簇中心）或“群体特征”来概括大量数据。\n",
    "*   **为后续任务提供基础：** 聚类的结果常常作为其他分析或任务的输入或预处理步骤（如分类、推荐、异常检测等）。\n",
    "\n",
    "**简单来说：聚类就是把“相似的东西”自动找出来并归为一堆的过程。**\n",
    "\n",
    "\n",
    "## 数据准备\n",
    "\n",
    "从 https://www.kaggle.com/datasets/dylanjcastillo/news-headlines-2024/ 下载新闻数据集，news_data_dedup.csv 并将其放入当前目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 创建虚拟环境（命名为 milvus_env，你可以自定义）\n",
    "! python -m venv milvus_env\n",
    "\n",
    "# 激活虚拟环境（Windows）\n",
    "! milvus_env\\Scripts\\activate\n",
    "\n",
    "# 激活后，你的终端提示符前会出现 (milvus_env)，表示已进入虚拟环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的命令，安装所需的依赖项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T07:27:56.715846Z",
     "iopub.status.busy": "2025-04-02T07:27:56.715575Z",
     "iopub.status.idle": "2025-04-02T07:28:19.571427Z",
     "shell.execute_reply": "2025-04-02T07:28:19.570863Z",
     "shell.execute_reply.started": "2025-04-02T07:27:56.715830Z"
    },
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install pandas python-dotenv sentence-transformers pymilvus modelscope hdbscan umap-learn plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如需加速国内下载，可加 -i 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install pandas python-dotenv sentence-transformers pymilvus modelscope hdbscan umap-learn plotly -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取 Embeddings 至 Milvus\n",
    "我们将使用 Milvus 创建一个 Collections，并使用 BGE-M3 模型提取密集嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-02T07:50:05.150469Z",
     "iopub.status.busy": "2025-04-02T07:50:05.150168Z",
     "iopub.status.idle": "2025-04-02T07:53:43.032601Z",
     "shell.execute_reply": "2025-04-02T07:53:43.032142Z",
     "shell.execute_reply.started": "2025-04-02T07:50:05.150451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import FieldSchema, Collection, connections, CollectionSchema, DataType\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_csv(\"news_data_dedup.csv\")\n",
    "\n",
    "# 使用列表推导式，将dataFame的title和description列拼接成一个列表，每个字符串由标题和描述组成\n",
    "docs = [\n",
    "    f\"{title}\\n{description}\" for title, description in zip(df.title, df.description)\n",
    "]\n",
    "\n",
    "# 使用modelscope下载模型（不推荐，有可能会很慢）\n",
    "# model_path = snapshot_download('BAAI/bge-m3', revision='master')\n",
    "# 对于手动下载的小伙伴，请将下面的路径替换为你下载模型的路径\n",
    "model = SentenceTransformer(r'C:\\Users\\xxxxxx\\.cache\\modelscope\\hub\\models\\BAAI\\bge-m3')\n",
    "# ...existing code...\n",
    "embeddings = model.encode(docs)  # 生成嵌入向量\n",
    "\n",
    "# 创建连接到 Milvus 数据库\n",
    "connections.connect(uri=\"http://localhost:19530\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建数据结构\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n",
    "    ), \n",
    "    FieldSchema(\n",
    "        name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1024\n",
    "    ), \n",
    "    FieldSchema(\n",
    "        name=\"text\", dtype=DataType.VARCHAR, max_length=65535\n",
    "    ), \n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, description=\"Embedding collection\")\n",
    "\n",
    "# 创建集合，news_data为集合名称\n",
    "collection = Collection(name=\"news_data\", schema=schema)\n",
    "\n",
    "# 插入数据\n",
    "for doc, embedding in zip(docs, embeddings):\n",
    "    collection.insert({\"text\": doc, \"embedding\": embedding})\n",
    "    print(doc)\n",
    "\n",
    "# 创建索引\n",
    "# 注意：索引创建可能需要一些时间，具体取决于数据量和服务器资源,\n",
    "# 如果你后续要进行查询，注意\"metric_type\": \"L2\"，查询的参数配置也要是L2\n",
    "index_params = {\"index_type\": \"FLAT\", \"metric_type\": \"L2\", \"params\": {}}\n",
    "\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "# 刷新集合，确保所有数据都被写入\n",
    "collection.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为 HDBSCAN 构建距离矩阵\n",
    "HDBSCAN 需要计算点与点之间的距离来进行聚类，计算量很大。由于远处的点对聚类分配的影响较小，我们可以通过计算前 k 个近邻来提高效率。在本例中，我们使用的是 FLAT 索引，但对于大规模数据集，Milvus 支持更高级的索引方法来加速搜索过程。 首先，我们需要获取一个迭代器来迭代之前创建的 Milvus Collections。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T07:55:46.834233Z",
     "iopub.status.busy": "2025-04-02T07:55:46.833893Z",
     "iopub.status.idle": "2025-04-02T07:55:55.468938Z",
     "shell.execute_reply": "2025-04-02T07:55:55.468339Z",
     "shell.execute_reply.started": "2025-04-02T07:55:46.834215Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 15:55:55,466 [WARNING][__setup_ts_by_request]: failed to get mvccTs from milvus server, use client-side ts instead (iterator.py:258)\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from umap import UMAP\n",
    "from pymilvus import Collection\n",
    "\n",
    "collection = Collection(name=\"news_data\")\n",
    "collection.load()\n",
    "\n",
    "# 查询迭代器\n",
    "# batch_size为每次查询的条数\n",
    "# expr为查询条件\n",
    "# output_fields为输出的字段\n",
    "iterator = collection.query_iterator(\n",
    "    batch_size=10, expr=\"id > 0\", output_fields=[\"id\", \"embedding\"]\n",
    ")\n",
    "\n",
    "# L2表示欧氏距离 作为相似性度量方法\n",
    "# nprobe表示查询时的探测数 决定了搜索的精度和速度\n",
    "search_params = {\n",
    "    # 这里的L2表示欧氏距离，要和你构建索引时的metric_type保持一致\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nprobe\": 10},\n",
    "}  \n",
    "ids = []\n",
    "dist = {}\n",
    "\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类操作\n",
    "下面我们来演示如何使用HDBScan进行聚类操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T07:55:58.823984Z",
     "iopub.status.busy": "2025-04-02T07:55:58.823318Z",
     "iopub.status.idle": "2025-04-02T07:56:00.619188Z",
     "shell.execute_reply": "2025-04-02T07:56:00.618739Z",
     "shell.execute_reply.started": "2025-04-02T07:55:58.823959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # 提取每条数据的id和embedding，id存入ids列表中\n",
    "    batch = iterator.next()\n",
    "    batch_ids = [data[\"id\"] for data in batch]\n",
    "    ids.extend(batch_ids)\n",
    "    # 将每条数据的embedding存入embeddings列表中 \n",
    "    query_vectors = [data[\"embedding\"] for data in batch]\n",
    "    embeddings.extend(query_vectors)\n",
    "    # 现在获取到了ids列表和embeddings列表，我们可以使用hdbscan进行聚类操作了\n",
    "\n",
    "\n",
    "    # 使用milvus的搜索\n",
    "    results = collection.search(\n",
    "        data=query_vectors,\n",
    "        limit=50,\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        output_fields=[\"id\"],\n",
    "    )\n",
    "    # 搜索结果存入dist字典中，键为batch_id，值为一个列表，列表中每个元素为一个元组，元组的第一个元素为id，第二个元素为距离（与该向量最相似的ID及其距离）\n",
    "    for i, batch_id in enumerate(batch_ids):\n",
    "        dist[batch_id] = []\n",
    "        for result in results[i]:\n",
    "            dist[batch_id].append((result.id, result.distance))\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        break\n",
    "\n",
    "# 构建距离矩阵\n",
    "# ids2index是一个字典，键为id，值为该id在ids列表中的索引\n",
    "ids2index = {}\n",
    "\n",
    "for id in dist:\n",
    "    ids2index[id] = len(ids2index)\n",
    "\n",
    "# dist_metric是二维距离矩阵\n",
    "dist_metric = np.full((len(ids), len(ids)), np.inf, dtype=np.float64)\n",
    "\n",
    "# 根据字典中的搜索结果填充距离矩阵，表示meita_id和batch_id之间的距离\n",
    "for id in dist:\n",
    "    for result in dist[id]:\n",
    "        dist_metric[ids2index[id]][ids2index[result[0]]] = result[1]\n",
    "\n",
    "# 使用HDBSCAN进行聚类，min_samples为每个点的最小邻居数，min_cluster_size为每个簇的最小点数，metric为距离度量方法，precomputed表示使用预计算的距离矩阵\n",
    "h = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3, metric=\"precomputed\")\n",
    "hdb = h.fit(dist_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，HDBSCAN 聚类就完成了。我们可以获取一些数据并显示其聚类结果。请注意，有些数据不会被分配到任何聚类中，这意味着它们是噪音，因为它们位于某些稀疏区域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 UMAP 进行聚类可视化\n",
    "我们已经使用 HDBSCAN 对数据进行了聚类，并获得了每个数据点的标签。不过，利用一些可视化技术，我们可以获得聚类的全貌，以便进行直观分析。现在，我们将使用 UMAP 对聚类进行可视化。UMAP 是一种用于降维的高效方法，它在保留高维数据结构的同时，将其投影到低维空间，以便进行可视化或进一步分析。有了它，我们就能在二维或三维空间中可视化原始高维数据，并清楚地看到聚类。 在这里，我们再次遍历数据点，获取原始数据的 ID 和文本，然后使用 ploty 将数据点与这些元信息绘制成图，并用不同的颜色代表不同的聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T07:56:05.388207Z",
     "iopub.status.busy": "2025-04-02T07:56:05.387914Z",
     "iopub.status.idle": "2025-04-02T07:56:15.269196Z",
     "shell.execute_reply": "2025-04-02T07:56:15.268329Z",
     "shell.execute_reply.started": "2025-04-02T07:56:05.388189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# 设置plotly的渲染器\n",
    "# 可选择notebook、browser等\n",
    "# 设置为notebook，表示在Jupyter Notebook中显示图表\n",
    "# 设置为browser，表示在浏览器中显示图表\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# 创建一个UMAP对象\n",
    "# 用于将高维数据降维到二维空间。UMAP的参数包括：\n",
    "# n_components=2：降维到二维。\n",
    "# random_state=42：设置随机种子以确保结果可复现。\n",
    "# n_neighbors=80和min_dist=0.1：控制UMAP的局部和全局结构保留程度\n",
    "umap = UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "\n",
    "df_umap = (\n",
    "    # 创建降维后的DataFrame\n",
    "    # 使用UMAP对embeddings（假设是高维嵌入向量）进行降维，并将结果存储在一个Pandas DataFrame中，列名为x和y。随后：\n",
    "    # 添加一列cluster，其值为HDBSCAN聚类结果的标签（hdb.labels_）。\n",
    "    # 过滤掉噪声点（cluster == \"-1\"）。\n",
    "    # 按照cluster列对数据进行排序。\n",
    "    pd.DataFrame(umap.fit_transform(np.array(embeddings)), columns=[\"x\", \"y\"])\n",
    "    .assign(cluster=lambda df: hdb.labels_.astype(str))\n",
    "    .query('cluster != \"-1\"')\n",
    "    .sort_values(by=\"cluster\")\n",
    ")\n",
    "# 从milvus中批量的查询数据\n",
    "iterator = collection.query_iterator(\n",
    "    batch_size=10, expr=\"id > 0\", output_fields=[\"id\", \"text\"]\n",
    ")\n",
    "\n",
    "ids = []\n",
    "texts = []\n",
    "# 查询到的数据存储到列表中\n",
    "while True:\n",
    "    batch = iterator.next()\n",
    "    if len(batch) == 0:\n",
    "        break\n",
    "    batch_ids = [data[\"id\"] for data in batch]\n",
    "    batch_texts = [data[\"text\"] for data in batch]\n",
    "    ids.extend(batch_ids)\n",
    "    texts.extend(batch_texts)\n",
    "\n",
    "show_texts = [texts[i] for i in df_umap.index]\n",
    "\n",
    "df_umap[\"hover_text\"] = show_texts\n",
    "fig = px.scatter(\n",
    "    df_umap, x=\"x\", y=\"y\", color=\"cluster\", hover_data={\"hover_text\": True}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://milvus.io/docs/zh/v2.4.x/hdbscan_clustering_with_milvus.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
