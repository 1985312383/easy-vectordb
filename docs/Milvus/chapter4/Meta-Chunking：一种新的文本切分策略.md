# Meta-Chunking

文本分块在 RAG 中起着关键作用，低效的分块策略会导致上下文不完整或包含过多无关信息，从而损害问答系统(QA)的性能(yu2023chain)。

在RAG的流程中，文本分块往往是容易被忽视的关键环节，这篇论文中提出Meta-Chunking这一元分块框架，**通过识别最优分割点与保留全局信息的双重策略，专门提升分块质量**。首先突破基于相似度的分块局限，我们利用LLM的能力，设计了两种基于不确定性的自适应分块技术**Perplexity Chunking and Margin Sampling Chunking**针对不同文本的固有复杂性，我们通过动态合并实现元分块，在细粒度与粗粒度文本分块间取得平衡。此外，我们建立了全局信息补偿机制，包含两阶段层次化摘要生成流程与聚焦缺失反思、精修和补全的三阶段文本块重写过程。这些组件共同增强了文本块的语义完整性与上下文连贯性。



- Through lightweight chunking algorithm design, the logical analysis capability of LLMs is decoupled into computable the PPL features and MSP indicators, achieving identification of textual logical boundaries and dynamic balance of chunking granularity.

  • 通过轻量级分块算法设计，将 LLMs 的逻辑分析能力解耦为可计算的 PPL 特征和 MSP 指标，实现文本逻辑边界的识别与分块粒度的动态平衡。

- We establish a information compensation mechanism that collaboratively executes through a three-stage missing-aware rewriting process and a two-stage context-aware summary generation, repairing the semantic discontinuities in text chunks.

  • 我们建立了信息补偿机制，通过三阶段缺失感知重写流程与两阶段上下文感知摘要生成的协同执行，修复文本块中的语义断裂问题。

> 1. **PPL (Perplexity)**：困惑度是自然语言处理中一个常用的评价指标，用于衡量一个概率模型预测样本的能力。简单来说，它评估了语言模型在预测下一个词时的不确定性。较低的困惑度意味着模型对文本的理解更好，能够更准确地预测下一个词。因此，在轻量级分块算法设计中，PPL 特征可能被用来作为衡量文本逻辑连贯性的一个标准，以帮助识别文本的逻辑边界。
> 2. **MSP (Most Significant Passage)**：虽然“MSP”这个缩写在特定领域可能有多种含义，但在这里，我们可以假设它指的是“最重要段落”或“最具代表性片段”。在给定的语境下，MSP 指标可能是用于识别文本中最能代表其主要内容或主题的部分。通过计算每个分块的重要性或代表性，可以帮助实现分块粒度的动态平衡，确保每个分块既不过于细碎也不过于笼统，从而更好地保持原始文本的逻辑结构和信息完整性。

我们的方法基于一个核心理念：通过允许分块大小的可变性，更有效地捕捉并保持内容的逻辑完整性。这种动态调整的粒度控制确保每个分割后的块都包含完整且独立的思想表达，从而避免分割过程中出现逻辑链条断裂。这不仅提升了文档检索的相关性，同时也增强了内容的清晰度。

如图 1 所示，我们的方法融合了传统文本分块策略的优势，例如遵循预设的分块长度约束和确保句子结构完整性，同时增强了在分块过程中保证逻辑连贯性的能力。我们将通过分块获得的每个文本块称为元块（Meta-Chunk），它由段落中按顺序排列的句子集合组成。这些句子不仅具有语义相关性，更重要的是还包含深层次的语言逻辑联系，包括但不限于总分、并列、递进和例证等关系，如图 4 所示。通过观察发现，元块内部连续句子之间往往存在紧密的逻辑联系，但由于内容表征的差异性，这些句子会表现出较低的语义相似度。正如 qu2024semantic 所述，语义分块在多个实验范式中均未能展现出优势。 我们认为这一现象与语义相似度算法的原始理论建模意图密切相关。这些方法本质上是通过建模文本间的语义重叠程度，来量化两个段落之间或句子与段落之间的关联性。然而在微观层面，那些具有逻辑关联却表达不同内容的句子限制了其适用性。详细分析见附录 B。为解决上述问题，我们基于 LLMs 中的不确定性理论实施了以下策略。



![Refer to caption](https://arxiv.org/html/2410.12788v3/extracted/6463559/pic/kuangjia.png)

**核心目标：** 将长文本分割成多个**连贯**的语块 (`X1, X2, ..., Xk`)，每个语块由连续的句子组成 (`xi`)，并且最终语块的长度尽可能满足用户需求。
$$
\text{PPL}_M(x_i) = \sum_{k=1}^{K} \frac{\text{PPL}_M(t_k^i \,|\, t_{<k}^i, t_{<i})}{K}
$$
**整体流程详解：**

1. **句子分割 (Sentence Segmentation):**
   - 输入：原始长文本。
   - 处理：使用句子分割工具（如基于规则、NLP库NLTK/spaCy等）将文本分割成一个个**完整的句子**。
   - 输出：句子序列 `S = (x1, x2, ..., xn)`。其中 `xi` 代表第 `i` 个句子。`n` 是总句子数。
2. **逐句计算 PPL (Perplexity Calculation per Sentence):**
   - **目标：** 量化当前句子 `xi` 在给定其**之前所有文本内容** (`x1, x2, ..., xi-1`) 的上下文下，语言模型 (LLM `M`) 对其的“惊讶程度”或“预测难度”。
   - **PPL 定义 (回顾):** 困惑度 (Perplexity, PPL) 是衡量语言模型预测能力的一个指标。**PPL 越低，表示模型对当前词序列的出现越不感到“困惑”，即该序列在给定上下文中越“自然”、越“可预测”、越“连贯”。**
   - **公式 (1) 详解：**
     `PPL_M(xi) = ( Σ_{k=1}^{K} PPL_M(t_k^i | t_{<k}^i, t_{<i}) ) / K`
     - `xi`: 当前要计算 PPL 的第 `i` 个句子。
     - `K`: 句子 `xi` 中包含的 **token (词元)** 的总数量。例如，`xi` = “我喜欢苹果” 可能被分词成 `["我", "喜欢", "苹果"]`，那么 `K=3`。
     - `t_k^i`: 句子 `xi` 中的第 `k` 个 token。
     - `t_{<k}^i`: 句子 `xi` 中，排在 `t_k^i` **之前**的所有 token。即 `xi` 句子内部，`t_k^i` 的上文。
     - `t_{<i}`: 所有排在句子 `xi` **之前**的 token。即整个文档中，`xi` 之前的所有句子的所有 token。这是 `xi` 的**外部上文**，提供了最重要的语境。
     - `PPL_M(t_k^i | t_{<k}^i, t_{<i})`: **核心条件概率计算。** 这表示在给定以下两部分的条件下：
       - `t_{<k}^i` (句子 `xi` 内部，`t_k^i` 之前的 token)
       - `t_{<i}` (整个文档中，`xi` 之前的所有 token)
         语言模型 `M` 预测出 token `t_k^i` 的**困惑度**。更基础的计算通常是模型预测 `t_k^i` 的**概率** `P_M(t_k^i | t_{<k}^i, t_{<i})`。PPL 是概率的衍生度量（通常基于概率的对数和平均）。
     - **`Σ_{k=1}^{K} ... / K`**: 对句子 `xi` 中的 **每一个 token (`k` 从 1 到 `K`)** 计算其条件 PPL，然后将这 `K` 个值**求和后再除以 `K` (句子 token 总数)**。这得到了句子 `xi` 在整个上下文 (`t_{<i}`) 下的**平均 token 级困惑度**。**这个平均值 `PPL_M(xi)` 就代表了整个句子 `xi` 在给定前文 (`t_{<i}`) 下的连贯性/自然度分数。分数越低，句子 `xi` 与前文的衔接越流畅、越符合预期。**
3. **构建 PPL 序列 (PPL Sequence Construction):**
   - 处理：对序列 `S` 中的**每一个句子 `xi` (i 从 1 到 n)** 都执行步骤 2，计算其 `PPL_M(xi)`。
   - 输出：得到一个与句子序列对应的 PPL 值序列：`PPL_seq = (PPL_M(x1), PPL_M(x2), ..., PPL_M(xn))`。
   - **关键洞察：** 当一个句子 `xi` 与其前文 (`t_{<i}`) 在语义或话题上发生**转换或断裂**时，模型预测 `xi` 的难度会**增大**，导致 `PPL_M(xi)` **升高**。相反，如果 `xi` 与前文紧密相关，则 `PPL_M(xi)` 会相对**较低**。更重要的是，在**话题结束点或新话题开始点**附近，我们预期会看到 `PPL_seq` 中的**局部极小值 (local minima)**。这些极小值点标识了文本中相对**更自然、更连贯的潜在分割位置**。
4. **识别关键分割点 (Finding Segmentation Points - Minima Detection):**
   - 目标：分析 `PPL_seq`，找出可以作为潜在语块边界的点（主要是局部极小值点）。
   - **算法关注两类极小值点 (对于点 `i`, 即句子 `xi` 的位置)：**
     - **类型 1 (波谷):** `PPL_M(xi-1) > PPL_M(xi)` **且** `PPL_M(xi+1) > PPL_M(xi)` **且** (`PPL_M(xi-1) - PPL_M(xi) > θ` **或** `PPL_M(xi+1) - PPL_M(xi) > θ`)。
       - 解释：点 `i` 的 PPL 比它左边 (`i-1`) 和右边 (`i+1`) 邻居的 PPL 都低（形成一个谷底），并且**至少有一侧**的下降幅度（左邻居减当前值 或 右邻居减当前值）超过了预设的**阈值 `θ`**。`θ` 是一个可调参数，用于过滤掉微小的、可能由噪声引起的波动，只关注显著的连贯性转换点。
     - **类型 2 (左悬崖):** `PPL_M(xi-1) - PPL_M(xi) > θ` **且** `PPL_M(xi+1) == PPL_M(xi)`。
       - 解释：点 `i` 的 PPL 相比其**左邻居 (`i-1`)** 有一个**显著的下降**（下降幅度 > `θ`），而其**右邻居 (`i+1`)** 的 PPL 与它**持平**。这通常表示一个话题在 `xi` 处结束或发生剧变，而 `xi` 和 `xi+1` 可能属于同一个新单元的开头。
   - 输出：一个潜在分割点位置的集合 `B = {b1, b2, ..., bm}` (例如 `b1=3`, `b2=7`, `b3=12` 表示建议在句子3后、7后、12后进行分割)。这些点标识了文本中相对连贯的子单元（元块）之间的边界。
5. **生成元块 (Meta-Chunk Formation):**
   - 处理：根据分割点集合 `B` 将句子序列 `S` 切分开。
   - 输出：元块序列 `MC = (MC1, MC2, ..., MCp)`。
     - 每个元块 `MCj` 由**两个相邻分割点之间（或从开头到第一个分割点，或最后一个分割点到结尾）的所有连续句子**组成。
     - 例如，分割点 `B = {3, 7, 12}`, 句子总数 `n=15`，则：
       - `MC1 = (x1, x2, x3)` (句子1到句子3)
       - `MC2 = (x4, x5, x6, x7)` (句子4到句子7)
       - `MC3 = (x8, x9, x10, x11, x12)` (句子8到句子12)
       - `MC4 = (x13, x14, x15)` (句子13到句子15)
     - **元块 `MCj` 的特性：** 算法认为每个元块内部的句子在给定的上下文（前文）下，具有相对较高的连贯性（由 PPL 极小值点标识其边界）。它们是**基于文本自身语义和结构**划分出的**基础连贯单元**。**元块的长度是算法根据连贯性自动决定的，通常是不均匀的。**
6. **动态组合元块以满足长度需求 (Dynamic Combination for Final Chunks):**
   - **目标：** 用户通常对最终输出语块 (`X`) 有**最大长度限制 `L`** (例如，不超过 512 tokens)。元块 `MCj` 本身可能太短或太长。这一步将**连续的元块合并**，形成最终的语块 `Xk`，使得每个 `Xk` 的 token 总数尽可能接近但不超过 `L`。
   - **处理 (贪心策略示例):**
     - 初始化：从第一个元块 `MC1` 开始。
     - 当前合并组：`CurrentChunk = [MC1]`, `CurrentLength = len(MC1)` (token 数)。
     - 遍历后续元块 (`MC2`, `MC3`, ... `MCp`)：
       - 如果 `CurrentLength + len(MC_next)` <= `L`：
         - 将 `MC_next` 加入 `CurrentChunk`。
         - `CurrentLength = CurrentLength + len(MC_next)`。
       - 否则 (`CurrentLength + len(MC_next)` > `L`)：
         - 将当前的 `CurrentChunk` 输出为一个最终语块 `Xk`。
         - 重置 `CurrentChunk = [MC_next]`。
         - 重置 `CurrentLength = len(MC_next)`。
     - 处理完所有元块后，将剩余的 `CurrentChunk` 输出为最后一个 `Xk`。
   - **输出：** 最终语块序列 `X = (X1, X2, ..., Xk)`。
     - 每个 `Xk` 由一个或多个**连续的元块 (`MCj`)** 组成。
     - 每个 `Xk` 的总 token 长度 <= `L` (用户设定的最大长度)。
     - **关键：** 组合发生在**元块的边界处**。算法**不会**在一个元块 `MCj` 内部（即其连贯句子组内部）进行切割。**优先保证了最终语块 `Xk` 内部包含的是完整的、算法认为连贯的子单元 (元块)。** 牺牲的是最终块可能由多个元块组成，但避免了破坏基础的连贯单元。
7. **处理超长文本：KV 缓存机制 (Handling Very Long Texts: KV Cache):**
   - **问题：** 计算 `PPL_M(xi)` 需要模型记住和处理 `xi` 之前的所有 token (`t_{<i}`)。对于超长文本，这会超过 LLM 的最大上下文长度或 GPU 内存限制。
   - **解决方案：** 采用 **Key-Value (KV) 缓存** 并实施**滚动缓存策略**。
     - **KV 缓存：** Transformer 模型在计算注意力时会生成 Key (`K`) 和 Value (`V`) 矩阵，代表之前 token 的信息。缓存这些 `K` 和 `V` 可以避免在预测后续 token 时重新计算前面的所有层，大大加速自回归生成和 PPL 计算。
     - **滚动缓存策略 (Rolling Cache Strategy):**
       - 将整个文本按 token 分成多个**子序列** (Segment)。
       - 计算第一个子序列的 PPL 时，正常建立并保留其完整的 KV 缓存。
       - 当处理到后续子序列，并且**累计的 KV 缓存大小接近设备 (GPU) 内存上限或模型最大上下文长度**时：
         - 策略性地**丢弃最早的一部分子序列的 KV 缓存** (例如，丢弃最早 30% 的缓存)。
         - 保留最近、最相关的部分子序列的 KV 缓存 (例如，保留最近 70% 的缓存)。
       - 用剩余缓存 + 新 token 继续计算后续句子的 PPL。
   - **权衡：** 丢弃部分历史缓存会损失一些远距离的上下文信息，可能轻微影响后续句子 PPL 计算的绝对准确性。但这种方法**在内存限制和保持足够连贯性上下文之间取得了平衡**，使得处理超长文本成为可能。通常，近期保留的上下文对于保持局部连贯性（即识别局部极小值点）已经足够重要。

**图 1 (Meta-Chunking Overview) 再解释：**

- **圆圈 (Circles):** 明确代表句子 `x1, x2, ..., xn`。大小不一表示句子长度不同。
- **垂直线 (Vertical Lines):** 位于某些句子之间，代表步骤 4 中通过分析 `PPL_seq` 找到的**潜在分割点**（极小值点）。它们是元块 (`MCj`) 的边界。
- **相同背景色区域 (Same Background Color):** 覆盖一组连续的句子。**每个这样的区域就是一个元块 (`MCj`)**。例如，黄色背景覆盖句子 `x1, x2, x3` 构成了元块 `MC1`；蓝色背景覆盖 `x4, x5, x6, x7` 构成了元块 `MC2`；绿色背景覆盖 `x8, x9, x10` 构成了元块 `MC3`。
- **"Dynamically combine meta chunks" 箭头:** 表示步骤 6 的过程。图中可能显示：
  - 元块 `MC1` (黄色) 被单独作为一个最终语块 `X1`（假设其长度已接近 `L`）。
  - 元块 `MC2` (蓝色) 和 `MC3` (绿色) 被**组合**起来形成最终语块 `X2`（因为单独 `MC2` 或 `MC3` 可能太短，组合后总长度接近 `L`）。
  - （图中未明确画出但逻辑包含）后续的元块（如 `MC4`）可能被组合或单独形成 `X3`, `X4` 等。
- **最终输出 (Final Output):** 最终提供给用户的是语块 `X1, X2, ...`，其边界就是组合后的元块边界（图中背景色的交界处）。`X1` 对应 `MC1` (黄色区域), `X2` 对应 `MC2+MC3` (蓝色+绿色区域)。

我们主要关注两类极小值点：当某点两侧的 PPL 均高于该点且至少一侧差值超过预设阈值 θ 时；或当左侧点与该点差值大于 θ 且右侧点等于该点值时。这些极小值被视为潜在语块边界。若文本超出 LLMs 或设备的处理范围，我们会策略性地引入键值（KV）缓存机制：先按 token 将文本分割为若干部分形成多个子序列，随着 PPL 计算的推进，当 GPU 内存即将超过服务器配置或 LLMs 最大上下文长度时，算法会适时移除先前部分文本的 KV 键值对，从而不过度牺牲上下文连贯性。