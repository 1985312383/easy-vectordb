## NLP概述

**引言**

- **定义**：NLP即自然语言处理，是计算机科学、人工智能以及语言学领域的分支学科，旨在让计算机能够理解、解释和生成人类的语言。

**NLP任务**

* 中文分词

  > 中文与英文不同，没有像英文一样词与词之间存在空格方便分词器进行识别。中文分词器是根据语义进行分词
  >
  > ​
  >
  > 比如：
  >
  > - 输入句子：`我爱自然语言处理`
  > - 分词结果：`["我", "爱", "自然语言", "处理"]`

* 子词切分

  > 对于未遇到的或者复杂的字词，子词即为可以描述这些字词的其他语句。
  >
  > ​
  >
  > 比如：
  >
  > - 输入词：`unhappiness`
  > - 子词切分结果：`["un", "happy", "ness"]`
  > - 输入词：`自然语言处理`
  > - 子词切分结果：`["自", "然", "语", "言", "处", "理"]` 或 `["自然", "语言", "处理"]`

* 词性标注

  > 对于某些词，确定这些词是属于什么类型的词语，是动词还是名词，形容词。
  >
  > ​
  >
  > 比如：
  >
  > - 输入句子：`我喜欢跑步`
  > - 词性标注结果：`["我/代词", "喜欢/动词", "跑步/动词"]`

* 文本分类

  > 文本分类是根据文本内容将其分配到一个或多个预定义类别中的任务。
  >
  > ​
  >
  > 比如：
  >
  > - 输入文本：`这部电影真的很好看！`
  > - 输出类别：`情感分析 -> 正面`

* 实体识别

  > 体识别是从文本中识别出具有特定意义的实体（如人名、地名、组织名、日期等）并分类。
  >
  > ​
  >
  > 比如：
  >
  > - 输入句子：`马云在杭州创立了阿里巴巴`
  > - 实体识别结果：`["马云/人物", "杭州/地点", "阿里巴巴/组织"]`

* 关系抽取

  > 关系抽取是识别文本中实体之间的关系，并进行结构化表示。
  >
  > ​
  >
  > 比如：
  >
  > - 输入句子：`马云在杭州创立了阿里巴巴`
  > - 关系抽取结果：`[("马云", "创立", "阿里巴巴"), ("阿里巴巴", "位于", "杭州")]`

* 文本摘要

  > 文本摘要是指从较长的文本中提取关键信息，生成简洁明了的总结。
  >
  > ​
  >
  > 比如：
  >
  > - 输入文本：`自然语言处理是人工智能的一个分支，研究计算机与人类语言之间的交互。它广泛应用于语音识别、机器翻译、自动问答等领域。`
  > - 摘要结果：`自然语言处理是人工智能的一部分，研究计算机与语言的交互，应用包括语音识别和机器翻译等。`

* 机器翻译

* 自动问答

  > 自动问答系统能够理解用户的问题，并从给定的知识库或上下文中找到准确答案。
  >
  > ​
  >
  > 比如：
  >
  > - 问题：`谁写了《红楼梦》？`
  > - 回答：`曹雪芹`

 **词向量**

​	将文本转化为向量的形式进行表示，可以对文本进行向量化然后与用户的问答进行相似度计算，评判是否相关，可以使用矩阵运算如特征值运行等方法优化向量化的表示，进一步处理相似度计算的效果。

**N-gram模型** 

​	是一种用来预测一句话中下一个词出现概率的方法基于马尔可夫假设：

- 如果 N=1，叫 unigram（一元），只考虑单个词出现的概率。
- 如果 N=2，叫 bigram（二元），每个词的概率只和前面那个词有关。
- 如果 N=3，叫 trigram（三元），每个词的概率和前面两个词有关。

  假设有句子 The quick brown fox

- 用 trigram（三元模型）时，计算 brown 这个词的概率时，会看它前面两个词（The 和 quick），
  即 P(brown | The, quick)。

N-gram 模型会把整句话的概率分解成多个 N-gram 的概率相乘。比如，用 trigram 模型，句子的概率就等于：`P(第3个词 | 第1、2个词) × P(第4个词 | 第2、3个词) × ...`

但是：
- N 越大，所需的数据就越多，很多词组合其实很少见或者没出现过，导致模型学不到这些情况（数据稀疏问题）
- 忽略了更远距离的词之间的联系，不能理解复杂语义

**Word2Vec**

​	词嵌入技术，利用词在文本中的上下文信息来捕捉词之间的语义关系，是的语义相似的词在向量空间的距离较近，这样在召回的时候，可以使用k-mean或者HDBSCAN算法来聚类这些相似度接近的词语或者语句，然后返回给模型，作为问答的高质量参考。

​	Word2Vec模型主要有两种架构：连续词袋模型CBOW(Continuous Bag of Words)是根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示；Skip-Gram模型与CBOW模型相反, 是利用目标词的向量表示计算上下文中的词向量. 实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好。

​	Word2Vec模型能够捕捉到词与词之间的语义关系，比如”国王“和“王后”在向量空间中的位置会比较接近，因为在大量文本中，它们通常会出现在相似的上下文中。

**ELMo**

​	Embeddings from Language Models，从大语言模型获取向量

* 首先在大规模的语料库训练向量化模型，然后进行微调，这样之后你会获得到更加适合某个任务所需的词向量。
* 该方法能够获取到词汇的上下文信息，生成更加合适的词向量



